{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG - Deep Deterministic Policy Gradients Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import mujoco_py\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG structure\n",
    "The algorithm consists of 4 networks. \n",
    "\n",
    "- Q Network\n",
    "- Deterministic policy function\n",
    "- target Q Network\n",
    "- target policy network\n",
    "\n",
    "Using the same principle as described in DQN, \n",
    "online networks are used for direct experiences sampled randomly from the replay buffer. \n",
    "Target networks are updated every N steps. The difference with DQN is that instead of executing hard update DDPG uses soft one.\n",
    "\n",
    "DDPG allows to work with continues output, when vanila DQN is not suited for such task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "# env = gym.make('Ant-v2')\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action = np.random.random_sample(env.action_space.shape[0])\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "\n",
    "print(f'Total reward {total_reward}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Network\n",
    "\n",
    "Actor network is used for constant action on environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs, outputs, hidden_layers=(128, 256)):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, hidden_layers[0])\n",
    "        f1 = 1 / np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "\n",
    "        self.bn1 = nn.LayerNorm(hidden_layers[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_norms = nn.ModuleList()\n",
    "        for i in range(len(hidden_layers)-1):\n",
    "            self.hidden_layers.append(\n",
    "                nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n",
    "            fn = 1 / np.sqrt(self.hidden_layers[i].weight.data.size()[0])\n",
    "            nn.init.uniform_(self.hidden_layers[i].weight.data, -fn, fn)\n",
    "            nn.init.uniform_(self.hidden_layers[i].bias.data, -fn, fn)\n",
    "            self.hidden_norms.append(nn.LayerNorm(hidden_layers[i+1]))\n",
    "\n",
    "        self.output = nn.Linear(hidden_layers[-1], outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = f.relu(self.bn1(self.fc1(X)))\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            X = f.relu(self.hidden_norms[i](self.hidden_layers[i](X)))\n",
    "\n",
    "        return torch.tanh(self.output(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Network\n",
    "\n",
    "Critic network is used for criticizing Actor by soft updates every once in awhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs, outputs, hidden_layers=(128, 256)):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, hidden_layers[0])\n",
    "        f1 = 1 / np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "\n",
    "        self.bn1 = nn.LayerNorm(hidden_layers[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_norms = nn.ModuleList()\n",
    "        for i in range(len(hidden_layers)-1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n",
    "            fn = 1 / np.sqrt(self.hidden_layers[i].weight.data.size()[0])\n",
    "            nn.init.uniform_(self.hidden_layers[i].weight.data, -fn, fn)\n",
    "            nn.init.uniform_(self.hidden_layers[i].bias.data, -fn, fn)\n",
    "            self.hidden_norms.append(nn.LayerNorm(hidden_layers[i+1]))\n",
    "        self.output = nn.Linear(hidden_layers[-1], outputs)\n",
    "\n",
    "    def forward(self, X1, X2):\n",
    "        X = torch.cat([X1, X2], 1) # concatenate states and actions by columns\n",
    "        X = f.relu(self.bn1(self.fc1(X)))\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            X = f.relu(self.hidden_norms[i](self.hidden_layers[i](X)))\n",
    "        \n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "Replay Buffer serves as a storage for sampling data points. Samples are drawn randomly to achieve IID - Independent and Identical Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer():\n",
    "\n",
    "    def __init__(self, cap=128):\n",
    "        self.buffer = deque(maxlen=cap)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        exp = (state, action, np.array([reward]), next_state, done)\n",
    "        self.buffer.append(exp)\n",
    "        \n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.vstack(states), np.vstack(actions), np.vstack(rewards), np.vstack(next_states), np.vstack(dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PER - Prioratized Experience Replay\n",
    "\n",
    "To ensure better datapoints used for training PER is a recommended approach.\n",
    "There are a few implementation of Replay Buffer that satisfy PER. For this demo OpenAI Segment Tree based approach was chosen.\n",
    "Extracted from https://raw.githubusercontent.com/openai/baselines/master/baselines/common/segment_tree.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SumTree\n",
    "# a binary tree data structure where the parentâ€™s value is the sum of its children\n",
    "\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        print(f'propagate {parent}; change {change}; parent any {np.any(parent)}')\n",
    "        if parent > 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PER:\n",
    "\n",
    "    def __init__(self, cap=128, alpha=0.6, beta=0.4, beta_increment=1e-3, eps=1e-3, batch_size=64):\n",
    "        self.cap = cap\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_increment = beta_increment\n",
    "        self.eps=eps\n",
    "        self.batch_size = batch_size\n",
    "        self.tree = SumTree(cap)\n",
    "\n",
    "    def add(self, error, experience):\n",
    "        state, action, reward, next_state, done = experience\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, (state, action, np.array([reward]), next_state, done))\n",
    "\n",
    "    def sample(self):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / self.batch_size\n",
    "        priorities = []\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment])\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries *\n",
    "                             sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.vstack(states), np.vstack(actions), np.vstack(rewards), np.vstack(next_states), np.vstack(dones), np.array(idxs, dtype=np.int16), np.array(is_weight)\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (np.abs(error) + self.eps) ** self.alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tree.n_entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ornstein-Ulhenbeck Process\n",
    "# Taken from #https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n",
    "class OUNoise(object):\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[0]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "    @staticmethod\n",
    "    def hard_update(target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, net_dim=(256, 256), buffer_size=50_000, batch_size=64, gamma=0.99, tau=1e-2, actor_lr=1e-4, critic_lr=1e-3, eps=1e-6, device='cpu'):\n",
    "        self.actor = Actor(state_size, action_size, hidden_layers=net_dim)\n",
    "        self.actor_target = Actor(\n",
    "            state_size, action_size, hidden_layers=net_dim)\n",
    "\n",
    "        self.critic = Critic(state_size + action_size,\n",
    "                             action_size, hidden_layers=net_dim)\n",
    "        self.critic_target = Critic(\n",
    "            state_size + action_size, action_size, hidden_layers=net_dim)\n",
    "\n",
    "        Utils.hard_update(self.actor_target, self.actor)\n",
    "        Utils.hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        # ReplayBuffer(cap=buffer_size)\n",
    "        self.buffer = PER(cap=buffer_size, batch_size=batch_size)\n",
    "        self.eps = eps\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(\n",
    "            self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.device = torch.device(device)\n",
    "        self.actor.to(self.device)\n",
    "        self.actor_target.to(self.device)\n",
    "        self.critic.to(self.device)\n",
    "        self.critic_target.to(self.device)\n",
    "\n",
    "    def step(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, _, weights, indices = self.buffer.sample()\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
    "        weights = torch.from_numpy(weights).float().to(\n",
    "            self.device).reshape(-1, 1)\n",
    "        indices = indices.astype(dtype=np.int16).reshape(-1, 1)\n",
    "\n",
    "        qval = self.critic.forward(states, actions)\n",
    "        next_actions = self.actor_target.forward(next_states)\n",
    "        nextQ = self.critic_target.forward(next_states, next_actions.detach())\n",
    "        qprime = rewards + self.gamma * nextQ * (1 - done)\n",
    "\n",
    "        TD_err = torch.abs(qprime - qval).detach().cpu().data.numpy()\n",
    "        for i in range(self.batch_size):\n",
    "            self.buffer.update(indices[i], TD_err[i])\n",
    "\n",
    "        # w_TD_err = torch.mul(TD_err, weights)\n",
    "        # zero_tensor = torch.zeros(w_TD_err.shape).to(self.device)\n",
    "        critic_loss = (weights * self.criterion(qval, qprime)).mean()\n",
    "        # critic_loss = self.criterion(w_TD_err, zero_tensor)\n",
    "\n",
    "        actor_loss = - \\\n",
    "            self.critic.forward(states, self.actor.forward(states)).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        Utils.soft_update(self.actor_target, self.actor, self.tau)\n",
    "        Utils.soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, state, target=False):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        action = self.actor.forward(\n",
    "            state) if not target else self.actor_target.forward(state)\n",
    "        return action.cpu().detach().numpy()[0]\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        target = self.actor(torch.from_numpy(\n",
    "            state).float().to(self.device)).data\n",
    "        old_val = target[0]\n",
    "        target_val = self.actor_target(torch.from_numpy(\n",
    "            next_state).float().to(self.device)).data\n",
    "        if done:\n",
    "            target[0] = reward\n",
    "        else:\n",
    "            target[0] = reward + self.eps * torch.max(target_val)\n",
    "\n",
    "        error = abs(old_val - target[0])\n",
    "        print(\n",
    "            f'Experience push {(state, action, reward, next_state, done)} error {error}')\n",
    "        self.buffer.add(error.detach().cpu().numpy(),\n",
    "                        (state, action, reward, next_state, done))\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.actor.state_dict(), path + '_actor.pth')\n",
    "        torch.save(self.critic.state_dict(), path + '_critic.pth')\n",
    "        torch.save(self.actor_target.state_dict(), path + '_actor_target.pth')\n",
    "        torch.save(self.critic_target.state_dict(),\n",
    "                   path + '_critic_target.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# traning routine\n",
    "EPISODES = 500\n",
    "\n",
    "# env = gym.make('InvertedDoublePendulum-v2') \n",
    "noise = OUNoise(env.action_space)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Using device {device}')\n",
    "agent = Agent(num_states, num_actions, buffer_size=500_000, device=device)\n",
    "\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "total_steps = 0\n",
    "for i in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    noise.reset()\n",
    "    ep_reward = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        total_steps += 1\n",
    "        action = agent.get_action(state)\n",
    "        action = noise.get_action(action, step)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # add new data to replay buffer\n",
    "        agent.push(state, action, reward, next_state, done)\n",
    "        if (total_steps + 1) % 20 == 0:\n",
    "            for _ in range(10):\n",
    "                agent.step()\n",
    "\n",
    "        ep_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(ep_reward)\n",
    "    avg_rewards.append(np.mean(rewards[-10:]))\n",
    "    print(f'\\rEpisode {i} reward {ep_reward:.3f} mean reward {avg_rewards[-1]:.3f}\\t', end='')\n",
    "#     if avg_rewards[-1] > -250:\n",
    "#         print(f'Early stop at episode {i}')\n",
    "#         break\n",
    "    \n",
    "plt.plot(rewards)\n",
    "plt.plot(avg_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "\n",
    "total_rewards = []\n",
    "for _ in range(10):\n",
    "    state = env.reset()\n",
    "    ep_reward = 0\n",
    "    while True:\n",
    "        # state = torch.from_numpy(state).float().unsqueeze(0) #.to(device)\n",
    "        action = agent.get_action(state, target=True)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        state = next_state\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f'\\rTotal reward: {ep_reward}', end='')\n",
    "    total_rewards.append(ep_reward)\n",
    "\n",
    "env.close()\n",
    "\n",
    "plt.plot(total_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9fd048f043f4012b7dbc9a5caf2bdc3a3b273ae9f5b56e59ab87291df9c49f43"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
